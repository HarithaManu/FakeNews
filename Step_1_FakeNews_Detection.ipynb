{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqp0MxveH1x",
        "outputId": "afbeef79-0fb7-4ded-eea8-1c1606d82468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fakenews_dataset\n",
            "--2025-03-24 10:43:07--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6e67d780-3ec5-11ea-95a2-63f4daa18a1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104307Z&X-Amz-Expires=300&X-Amz-Signature=221e097324889b2b286b8349a02c11a91a69d3245c0be533283e24529cc3bb89&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:43:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6e67d780-3ec5-11ea-95a2-63f4daa18a1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104307Z&X-Amz-Expires=300&X-Amz-Signature=221e097324889b2b286b8349a02c11a91a69d3245c0be533283e24529cc3bb89&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 142148858 (136M) [application/octet-stream]\n",
            "Saving to: ‘news.csv.zip’\n",
            "\n",
            "news.csv.zip        100%[===================>] 135.56M   167MB/s    in 0.8s    \n",
            "\n",
            "2025-03-24 10:43:08 (167 MB/s) - ‘news.csv.zip’ saved [142148858/142148858]\n",
            "\n",
            "--2025-03-24 10:43:09--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z01\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6c9e1400-3ec5-11ea-8eab-9942584ac3db?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104309Z&X-Amz-Expires=300&X-Amz-Signature=9f7bf3c0d95d7b3ba4ed6858392d8d9925d494a0f11ee67272334b3043b76bbf&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z01&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:43:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6c9e1400-3ec5-11ea-8eab-9942584ac3db?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104309Z&X-Amz-Expires=300&X-Amz-Signature=9f7bf3c0d95d7b3ba4ed6858392d8d9925d494a0f11ee67272334b3043b76bbf&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z01&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z01’\n",
            "\n",
            "news.csv.z01        100%[===================>]   1.00G  96.7MB/s    in 12s     \n",
            "\n",
            "2025-03-24 10:43:21 (83.6 MB/s) - ‘news.csv.z01’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:43:22--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z02\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6c9e1400-3ec5-11ea-9490-748980f2d9dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104322Z&X-Amz-Expires=300&X-Amz-Signature=f418396f69e80808dc687393bc47131e8cec2f1578bf6752b4b8e3076874a4e0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z02&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:43:22--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6c9e1400-3ec5-11ea-9490-748980f2d9dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104322Z&X-Amz-Expires=300&X-Amz-Signature=f418396f69e80808dc687393bc47131e8cec2f1578bf6752b4b8e3076874a4e0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z02&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z02’\n",
            "\n",
            "news.csv.z02        100%[===================>]   1.00G  2.81MB/s    in 18s     \n",
            "\n",
            "2025-03-24 10:43:40 (55.7 MB/s) - ‘news.csv.z02’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:43:41--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z03\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6d36aa80-3ec5-11ea-8443-269e7c452773?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104341Z&X-Amz-Expires=300&X-Amz-Signature=32fac3d084d7103425c076330536badb2387f4f4f026876254742f82f8872ac7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z03&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:43:41--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6d36aa80-3ec5-11ea-8443-269e7c452773?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104341Z&X-Amz-Expires=300&X-Amz-Signature=32fac3d084d7103425c076330536badb2387f4f4f026876254742f82f8872ac7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z03&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z03’\n",
            "\n",
            "news.csv.z03        100%[===================>]   1.00G   111MB/s    in 10s     \n",
            "\n",
            "2025-03-24 10:43:51 (102 MB/s) - ‘news.csv.z03’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:43:51--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z04\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-9781-2307f5ccfe10?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104351Z&X-Amz-Expires=300&X-Amz-Signature=6a60386f113cb18c56ef0759121ae3ef817b6bff38ab3445d90c758f0da9a848&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z04&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:43:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-9781-2307f5ccfe10?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104351Z&X-Amz-Expires=300&X-Amz-Signature=6a60386f113cb18c56ef0759121ae3ef817b6bff38ab3445d90c758f0da9a848&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z04&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z04’\n",
            "\n",
            "news.csv.z04        100%[===================>]   1.00G   111MB/s    in 8.6s    \n",
            "\n",
            "2025-03-24 10:44:00 (119 MB/s) - ‘news.csv.z04’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:44:00--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z05\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-9bc2-08b400ed986c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104400Z&X-Amz-Expires=300&X-Amz-Signature=855d1380ce3d671de171b3946258509947ecc8215e2f6472cbbe202e9f949da9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z05&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:44:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-9bc2-08b400ed986c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104400Z&X-Amz-Expires=300&X-Amz-Signature=855d1380ce3d671de171b3946258509947ecc8215e2f6472cbbe202e9f949da9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z05&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z05’\n",
            "\n",
            "news.csv.z05        100%[===================>]   1.00G   166MB/s    in 6.8s    \n",
            "\n",
            "2025-03-24 10:44:07 (151 MB/s) - ‘news.csv.z05’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:44:07--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z06\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8c22-10d01bb276eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104407Z&X-Amz-Expires=300&X-Amz-Signature=d7e9336dc94807a56484782321a593756c6a09ed7e5512a7401a5c6dd7217d91&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z06&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:44:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8c22-10d01bb276eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104407Z&X-Amz-Expires=300&X-Amz-Signature=d7e9336dc94807a56484782321a593756c6a09ed7e5512a7401a5c6dd7217d91&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z06&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z06’\n",
            "\n",
            "news.csv.z06        100%[===================>]   1.00G   126MB/s    in 8.7s    \n",
            "\n",
            "2025-03-24 10:44:16 (118 MB/s) - ‘news.csv.z06’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:44:16--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z07\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8ed2-22fb20977ce9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104416Z&X-Amz-Expires=300&X-Amz-Signature=7ddf822d064b7181c84438398d896cf23c81cd37e019243d1148d3b8c27491f3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z07&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:44:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8ed2-22fb20977ce9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104416Z&X-Amz-Expires=300&X-Amz-Signature=7ddf822d064b7181c84438398d896cf23c81cd37e019243d1148d3b8c27491f3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z07&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z07’\n",
            "\n",
            "news.csv.z07        100%[===================>]   1.00G   160MB/s    in 6.7s    \n",
            "\n",
            "2025-03-24 10:44:23 (153 MB/s) - ‘news.csv.z07’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:44:23--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z08\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8797-d906b8a7400d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104424Z&X-Amz-Expires=300&X-Amz-Signature=7e9a6f41fcaf73d0716bcbcf9b8e41040795160fcd0106bd8882330c94ecfa58&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z08&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:44:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6dcf4100-3ec5-11ea-8797-d906b8a7400d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104424Z&X-Amz-Expires=300&X-Amz-Signature=7e9a6f41fcaf73d0716bcbcf9b8e41040795160fcd0106bd8882330c94ecfa58&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z08&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z08’\n",
            "\n",
            "news.csv.z08        100%[===================>]   1.00G   166MB/s    in 8.5s    \n",
            "\n",
            "2025-03-24 10:44:32 (121 MB/s) - ‘news.csv.z08’ saved [1073741824/1073741824]\n",
            "\n",
            "--2025-03-24 10:44:32--  https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z09\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6e67d780-3ec5-11ea-8c20-5dfdd4a779f4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104432Z&X-Amz-Expires=300&X-Amz-Signature=e4c53376e570cb508abe345a99bcccc8ce95cd471da69ec33415a9e223a31f57&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z09&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 10:44:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/119894144/6e67d780-3ec5-11ea-8c20-5dfdd4a779f4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T104432Z&X-Amz-Expires=300&X-Amz-Signature=e4c53376e570cb508abe345a99bcccc8ce95cd471da69ec33415a9e223a31f57&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnews.csv.z09&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073741824 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.csv.z09’\n",
            "\n",
            "news.csv.z09        100%[===================>]   1.00G  99.0MB/s    in 8.7s    \n",
            "\n",
            "2025-03-24 10:44:41 (118 MB/s) - ‘news.csv.z09’ saved [1073741824/1073741824]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create a directory for the dataset\n",
        "dataset_dir = \"/content/fakenews_dataset\"\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Navigate to the dataset directory\n",
        "%cd \"$dataset_dir\"\n",
        "\n",
        "# Download all parts of the dataset\n",
        "!wget -c https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.zip\n",
        "for i in range(1, 10):\n",
        "    !wget -c https://github.com/several27/FakeNewsCorpus/releases/download/v1.0/news.csv.z0{i}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/fakenews_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHFhIY6ueWqh",
        "outputId": "e07f4ec7-b2e4-434e-f8c8-ce0650a22d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 9.2G\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z01\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z02\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z03\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z04\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z05\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z06\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z07\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z08\n",
            "-rw-r--r-- 1 root root 1.0G Dec  6  2021 news.csv.z09\n",
            "-rw-r--r-- 1 root root 136M Dec  6  2021 news.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the dataset directory\n",
        "%cd /content/fakenews_dataset\n",
        "\n",
        "# Merge the split files into one ZIP file\n",
        "!cat news.csv.z0* news.csv.zip > merged_news.zip\n",
        "\n",
        "# Unzip the merged file\n",
        "!unzip merged_news.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEmXI-unebHk",
        "outputId": "8a1318c5-a3bb-4608-85ba-a1e65d15ad0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fakenews_dataset\n",
            "Archive:  merged_news.zip\n",
            "error: End-of-centdir-64 signature not where expected (prepended bytes?)\n",
            "  (attempting to process anyway)\n",
            "warning [merged_news.zip]:  zipfile claims to be last disk of a multi-part archive;\n",
            "  attempting to process anyway, assuming all parts have been concatenated\n",
            "  together in order.  Expect \"errors\" and warnings...true multi-part support\n",
            "  doesn't exist yet (coming soon).\n",
            "warning [merged_news.zip]:  9663676416 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  9663676420\n",
            "  (attempting to re-compensate)\n",
            "  inflating: news_cleaned_2018_02_13.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path of the dataset\n",
        "dataset_path = \"/content/fakenews_dataset/news_cleaned_2018_02_13.csv\"\n",
        "\n",
        "# Estimate the chunk size based on system memory (for example, 1GB of data per chunk)\n",
        "chunk_size_bytes = 1 * 1024 * 1024 * 1024  # 1GB (you can adjust this based on your memory constraints)\n",
        "chunk_size = chunk_size_bytes // 100  # Adjust this to limit the number of rows loaded based on available RAM\n",
        "\n",
        "# The 10% of the data is approximately 2.7GB, so we need to load 10% of the total rows\n",
        "target_bytes = 2.7 * 1024 * 1024 * 1024  # Target size is 2.7GB (10% of the 27GB file)\n",
        "total_bytes_loaded = 0  # Track total bytes loaded\n",
        "\n",
        "# Create an empty list to store the processed chunks\n",
        "processed_chunks = []\n",
        "\n",
        "# Load data in chunks\n",
        "for chunk in pd.read_csv(dataset_path, usecols=['title', 'content'], chunksize=chunk_size):\n",
        "    # Get the size of the chunk in bytes\n",
        "    chunk_size_in_bytes = chunk.memory_usage(deep=True).sum()\n",
        "\n",
        "    # Update the total bytes loaded\n",
        "    total_bytes_loaded += chunk_size_in_bytes\n",
        "\n",
        "    # Process the chunk if we're still under the target size\n",
        "    if total_bytes_loaded > target_bytes:\n",
        "        remaining_bytes = target_bytes - (total_bytes_loaded - chunk_size_in_bytes)\n",
        "        chunk = chunk.head(remaining_bytes // chunk.memory_usage(deep=True).sum())  # Limit to remaining bytes\n",
        "        total_bytes_loaded = target_bytes  # We're done loading 10% of the data\n",
        "\n",
        "    # Process the chunk (for example, print the first few rows)\n",
        "    print(f\"Processing chunk of size {chunk_size_in_bytes / (1024 ** 2):.2f} MB\")\n",
        "    processed_chunks.append(chunk)\n",
        "\n",
        "    # Stop processing if we've loaded 10% of the dataset\n",
        "    if total_bytes_loaded >= target_bytes:\n",
        "        break\n",
        "\n",
        "# Combine all the processed chunks into a single DataFrame (if needed)\n",
        "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the final processed DataFrame\n",
        "final_df.head()\n"
      ],
      "metadata": {
        "id": "7Uy4ZLT0waR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = \"/content/fakenews_dataset/news_cleaned_2018_02_13.csv\"\n",
        "\n",
        "# Load only the relevant columns\n",
        "df = pd.read_csv(dataset_path, usecols=['title', 'content'], nrows=100000)\n",
        "\n",
        "# Display first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PgMBxgzDeisU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hps-> This is to extract the subset dataset which is of size less than 1GB.\n",
        "# can alter the value of nrows to more to\n",
        "# Define the output file path\n",
        "output_path = \"/content/fakenews_dataset/news_subset.csv\"\n",
        "\n",
        "# Save the first 100,000 rows to a new CSV file\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Saved 100,000 rows to {output_path}\")"
      ],
      "metadata": {
        "id": "b0GFJK_xq3Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the data preprocessing snippet.\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "total_data_loaded = 0  # Variable to accumulate the total size of loaded data\n",
        "\n",
        "def load_data_with_tensorflow(filepath, chunksize=100000): # hps-> chunk size can be altered accordingly.\n",
        "    global total_data_loaded\n",
        "\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        filepath,\n",
        "        batch_size=chunksize,\n",
        "        label_name='type',\n",
        "        select_columns=['content', 'type'],\n",
        "        num_epochs=1,\n",
        "        ignore_errors=True,\n",
        "        header=True\n",
        "    )\n",
        "\n",
        "    num_chunks = 0\n",
        "    for batch in tqdm(dataset.take(30), desc=\"Loading Data\"):\n",
        "        features_dict, labels = batch\n",
        "        features = features_dict['content']\n",
        "\n",
        "        # Convert to numpy and filter out empty rows\n",
        "        features_np = features.numpy()\n",
        "        labels_np = labels.numpy()\n",
        "\n",
        "        valid_indices = []\n",
        "\n",
        "        for i, (feature, label) in enumerate(zip(features_np, labels_np)):\n",
        "            # Check if both content and type columns are non-empty\n",
        "            if feature.strip() and label.strip():\n",
        "                valid_indices.append(i)\n",
        "\n",
        "        valid_features = features_np[valid_indices]\n",
        "        valid_labels = labels_np[valid_indices]\n",
        "\n",
        "        total_data_loaded += sum(len(feature) for feature in valid_features)\n",
        "        yield valid_features, valid_labels\n",
        "\n",
        "def preprocess_data(features, labels, stop_words, stemmer):\n",
        "    # Define a multiprocessing pool\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "\n",
        "    # Preprocess each feature in parallel\n",
        "    processed_results = pool.starmap(process_text, [(text, stop_words, stemmer) for text in features])\n",
        "\n",
        "    # Close the pool\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    processed_features = []\n",
        "    processed_labels = []\n",
        "    vocab_sizes = []  # List to store vocabulary sizes\n",
        "    vocab_sizes_after_filtering = []\n",
        "    urls_counts = []\n",
        "    dates_counts = []\n",
        "    numerics_counts = []\n",
        "    for result, label in zip(processed_results, labels):\n",
        "        if result:\n",
        "            processed_text, vocab_size, vocab_size_after_filtering,num_urls,num_dates, num_numerics= result  # Unpack the result tuple\n",
        "            processed_features.append(processed_text)\n",
        "            processed_labels.append(classify_news_type(label.decode('utf-8')))  # Decode label to string\n",
        "            vocab_sizes.append(vocab_size)\n",
        "            vocab_sizes_after_filtering.append(vocab_size_after_filtering)\n",
        "            urls_counts.append(num_urls)\n",
        "            dates_counts.append(num_dates)\n",
        "            numerics_counts.append(num_numerics)\n",
        "\n",
        "    return processed_features, processed_labels, vocab_sizes, vocab_sizes_after_filtering,urls_counts,dates_counts,numerics_counts\n",
        "\n",
        "def process_text(text, stop_words, stemmer):\n",
        "    if not text.strip():  # Check if the text is empty or contains only whitespace\n",
        "        return '', 0, 0, 0  # Return empty strings and counts if the text is empty\n",
        "\n",
        "    # Decode the bytes-like object to a string\n",
        "    text = text.decode('utf-8')\n",
        "\n",
        "    # Count URLs in the content\n",
        "    num_urls = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "\n",
        "    # Count dates in the content\n",
        "\n",
        "    dates = re.findall(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', text) # Example date format: 01/01/2022\n",
        "    num_dates = len(dates)\n",
        "\n",
        "    # Count numeric values in the content\n",
        "\n",
        "    numerics = re.findall(r'\\b\\d+\\b', text)  # Extracts integers\n",
        "    num_numerics = len(numerics)\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vocab_size = len(set(tokens))\n",
        "    processed_text = ' '.join(tokens)\n",
        "    vocab_size_after_filtering = len(set(tokens))\n",
        "    return processed_text, vocab_size, vocab_size_after_filtering,num_urls,num_dates,num_numerics\n",
        "\n",
        "def classify_news_type(news_type):\n",
        "    fake_types = ['fake', 'conspiracy','unreliable','satire','bias']\n",
        "    reliable_types = ['political', 'reliable']\n",
        "    if news_type in fake_types:\n",
        "        return 'Fake'\n",
        "    elif news_type in reliable_types:\n",
        "        return 'Reliable'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def get_tokens_size_on_disk(tokens, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Stemmed Tokens'])  # Write header\n",
        "        for token in tokens:\n",
        "            writer.writerow([token])  # Write each stemmed token to a separate row\n",
        "    return os.path.getsize(filename)\n",
        "\n",
        "def bytes_to_gb(size_in_bytes):\n",
        "    return size_in_bytes / (1024 ** 3)\n",
        "\n",
        "def remove_stopwords_and_stem(text, stop_words, stemmer):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and perform stemming\n",
        "    filtered_stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return filtered_stemmed_tokens\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    filepath = 'news.csv'  # Adjust the file path as needed\n",
        "    chunksize = 60000\n",
        "\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    dataset = load_data_with_tensorflow(filepath, chunksize=chunksize)\n",
        "\n",
        "    X_chunks = []\n",
        "    y_chunks = []\n",
        "    total_vocab_size = 0  # Initialize total_vocab_size\n",
        "    total_vocab_size_after_filtering = 0\n",
        "    total_tokens_size_on_disk = 0  # Initialize total size of tokens on disk\n",
        "\n",
        "\n",
        "    removed_rows_per_chunk = {}  # Dictionary to store the number of removed rows per chunk\n",
        "\n",
        "\n",
        "    for chunk_num, (features, labels) in enumerate(dataset, start=1):\n",
        "        removed_rows = 0\n",
        "\n",
        "        processed_data = preprocess_data(features, labels, stop_words, stemmer)\n",
        "        processed_features, processed_labels, vocab_sizes, vocab_sizes_after_filtering,urls_count, dates_count, numerics_count  = processed_data\n",
        "\n",
        "        if not processed_features:\n",
        "            print(f\"No features loaded in chunk {chunk_num}.\")\n",
        "        if not processed_labels:\n",
        "            print(f\"No labels loaded in chunk {chunk_num}.\")\n",
        "\n",
        "        # Calculate and print the size of tokens on disk\n",
        "        for text in processed_features:\n",
        "            tokens = text.split()\n",
        "            tokens_filename = \"tokens.csv\"\n",
        "            tokens_size_on_disk = get_tokens_size_on_disk(tokens, tokens_filename)\n",
        "            total_tokens_size_on_disk += tokens_size_on_disk\n",
        "\n",
        "\n",
        "        fake_counts = processed_labels.count('Fake')\n",
        "        reliable_counts = processed_labels.count('Reliable')\n",
        "        neutral_counts = processed_labels.count('Neutral')\n",
        "\n",
        "        # Count the number of removed rows\n",
        "        removed_rows = chunksize - len(processed_features)\n",
        "        removed_rows_per_chunk[chunk_num] = {\n",
        "        'removed_rows': removed_rows,\n",
        "        'urls_count': sum(urls_count),\n",
        "        'dates_count': sum(dates_count),\n",
        "        'numerics_count': sum(numerics_count),\n",
        "        'fake': fake_counts,\n",
        "        'reliable': reliable_counts,\n",
        "        'neutral': neutral_counts\n",
        "\n",
        "         }\n",
        "\n",
        "        X_chunks.extend(processed_features)\n",
        "        y_chunks.extend(processed_labels)\n",
        "\n",
        "\n",
        "        total_vocab_size += sum(vocab_sizes)  # Accumulate the total vocabulary size\n",
        "        total_vocab_size_after_filtering += sum(vocab_sizes_after_filtering)  # Accumulate the total vocabulary size\n",
        "\n",
        "    print(f\"Total Vocab Size: {total_vocab_size} \")\n",
        "    print(f\"Total Vocab Size after stemming: {total_vocab_size_after_filtering}\")\n",
        "\n",
        "    if not X_chunks or not y_chunks:\n",
        "        print(\"No data loaded. Please check the dataset or adjust parameters.\")\n",
        "        return\n",
        "\n",
        "    X = X_chunks  # No need to convert to NumPy array\n",
        "    y = y_chunks\n",
        "\n",
        "\n",
        "   # Perform stemming\n",
        "    all_text = ' '.join(X)\n",
        "\n",
        "    # Tokenize the text excluding numeric values and non-word tokens\n",
        "    word_tokens = word_tokenize(all_text.lower())\n",
        "    filtered_words = [word for word in word_tokens if word.isalpha() and not word.isdigit()]\n",
        "\n",
        "    # Determine the 100 most frequent words excluding numeric values\n",
        "    word_freq = Counter(filtered_words)\n",
        "    top_10000_words = word_freq.most_common(10000)\n",
        "    top_1000_words = word_freq.most_common(1000)\n",
        "    top_100_words = word_freq.most_common(100)\n",
        "\n",
        "\n",
        "\n",
        "    # Extract words and frequencies\n",
        "    words_100 = [word[0] for word in top_100_words]\n",
        "    frequencies_100 = [word[1] for word in top_100_words]\n",
        "\n",
        "    # Extract words and frequencies\n",
        "    words_1000 = [word[0] for word in top_1000_words]\n",
        "    frequencies_1000 = [word[1] for word in top_1000_words]\n",
        "\n",
        "    # Extract words and frequencies\n",
        "    words_10000 = [word[0] for word in top_10000_words]\n",
        "    frequencies_10000 = [word[1] for word in top_10000_words]\n",
        "\n",
        "    # Calculate ranks\n",
        "\n",
        "    # Plotting 100\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.bar(words_100[:100], frequencies_100[:100], color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Top 100 Words and Their Frequencies')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Top 100 most frequent words (excluding numeric values) in the whole dataset:\")\n",
        "    for word, freq in top_100_words:\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "        # Plotting 1000\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.bar(words_1000[:1000], frequencies_1000[:1000], color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Top 1000 Words and Their Frequencies')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        # Plotting 10000\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.bar(words_10000[:10000], frequencies_10000[:10000], color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Top 10000 Words and Their Frequencies')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print total data loaded\n",
        "    print(f\"Total data loaded: {total_data_loaded / (1024 ** 3):.6f} GB\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Total size of all tokens on disk: {bytes_to_gb(total_tokens_size_on_disk):.6f} GB\")\n",
        "\n",
        "    # Classify news types\n",
        "    for label in set(y):\n",
        "        news_type = classify_news_type(label)  # No need to decode label\n",
        "        print(f\"Label: {label}, News Type: {news_type}\")\n",
        "\n",
        "    # Print removed rows per chunk\n",
        "    print(\"\\nRemoved Rows per Chunk:\")\n",
        "    for chunk_num, data in removed_rows_per_chunk.items():\n",
        "        print(f\"Chunk {chunk_num}:\")\n",
        "        print(f\"  Removed rows: {data['removed_rows']}\")\n",
        "        print(f\"  URLs count: {data['urls_count']}\")\n",
        "        print(f\"  Dates count: {data['dates_count']}\")\n",
        "        print(f\"  Numeric values count: {data['numerics_count']}\")\n",
        "        print(f\"  fake: {data['fake']}\")\n",
        "        print(f\"  reliable: {data['reliable']}\")\n",
        "        print(f\"  neutral: {data['neutral']}\")\n",
        "\n",
        "    # Extract data for plotting\n",
        "    chunks = list(removed_rows_per_chunk.keys())\n",
        "    removed_rows = [data['removed_rows'] for data in removed_rows_per_chunk.values()]\n",
        "    urls_counts = [data['urls_count'] for data in removed_rows_per_chunk.values()]\n",
        "    dates_counts = [data['dates_count'] for data in removed_rows_per_chunk.values()]\n",
        "    numeric_values_counts = [data['numerics_count'] for data in removed_rows_per_chunk.values()]\n",
        "    fake_counts = [data['fake'] for data in removed_rows_per_chunk.values()]\n",
        "    reliable_counts = [data['reliable'] for data in removed_rows_per_chunk.values()]\n",
        "    neutral_counts = [data['neutral'] for data in removed_rows_per_chunk.values()]\n",
        "\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(chunks, removed_rows, label='Removed Rows', color='skyblue', alpha=0.9)\n",
        "    plt.bar(chunks, urls_counts, bottom=removed_rows, label='URLs Count', color='orange')\n",
        "    plt.bar(chunks, dates_counts, bottom=[i+j for i,j in zip(removed_rows, urls_counts)], label='Dates Count', color='green')\n",
        "    plt.bar(chunks, numeric_values_counts, bottom=[i+j+k for i,j,k in zip(removed_rows, urls_counts, dates_counts)], label='Numeric Values Count', color='red')\n",
        "    plt.xlabel('Chunk')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Counts of Removed Rows, URLs, Dates, Numeric Values, per Chunk')\n",
        "    plt.legend()\n",
        "    plt.xticks(chunks)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3ScxiwROeox8",
        "outputId": "9e69bcb5-dec7-4863-825f-eae793039236"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No files match `file_pattern` news.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-54026489d276>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-54026489d276>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mremoved_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-54026489d276>\u001b[0m in \u001b[0;36mload_data_with_tensorflow\u001b[0;34m(filepath, chunksize)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtotal_data_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     dataset = tf.data.experimental.make_csv_dataset(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36mmake_csv_dataset_v2\u001b[0;34m(file_pattern, batch_size, column_names, column_defaults, label_name, select_columns, field_delim, use_quote_delim, na_value, header, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, num_parallel_reads, sloppy, num_rows_for_inference, compression_type, ignore_errors, encoding)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;31m# Create dataset of all matching filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m   \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_file_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36m_get_file_names\u001b[0;34m(file_pattern, shuffle)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No files match `file_pattern` {file_pattern}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m   \u001b[0;31m# Sort files so it will be deterministic for unit tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No files match `file_pattern` news.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDP4bxtb1z2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}